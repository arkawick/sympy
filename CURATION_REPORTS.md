# AI Curation Reports - Implementation Summary

## Overview

The enhanced workflow now generates **TWO types of AI-powered curation reports**:

### 1. Main ORT Curation Report (`curation-report-main.html`)
**Generated by:** `ort_curation_script_html.py` (your original script)

**Purpose:** Comprehensive compliance analysis of all ORT results

**Features:**
- Executive summary with overall project status
- Complete license inventory and distribution
- Package-by-package analysis
- Risk assessment (high/medium/low priority issues)
- Actionable recommendations
- Go/No-Go compliance verdict
- Beautiful gradient styling with LTTS branding

**When it runs:**
- Always runs if ORT analyzer completes successfully
- Requires `AZURE_OPENAI_API_KEY` secret
- Uses Azure OpenAI model: `gpt-4.1-mini`

### 2. License Conflict Analysis Report (`curation-report-conflicts.html`)
**Generated by:** `enhanced_ai_curation.py` (new multi-tool script)

**Purpose:** Deep analysis of license conflicts and uncertain packages

**Features:**
- Multi-tool comparison (ORT + ScanCode + SPDX)
- Conflict resolution recommendations
- Risk-level assessment for each conflict
- Specific actionable steps for each package
- Links to source repositories and documentation

**When it runs:**
- Only runs if uncertain packages are detected
- Uses enhanced SPDX if ScanCode scanning completed
- Requires both `AZURE_OPENAI_API_KEY` and `AZURE_OPENAI_ENDPOINT` secrets
- Uses Azure OpenAI model: `gpt-4`

## Workflow Stages

### Stage 5a: Main ORT Curation
```bash
python3 ort_curation_script_html.py
```
- Analyzes `ort-results/analyzer/analyzer-result.yml`
- Generates timestamped report: `curation-report-YYYYMMDD-HHMMSS.html`
- Copies to `curation-report-main.html` for deployment

### Stage 5b: Enhanced Conflict Analysis
```bash
python3 enhanced_ai_curation.py \
  --ort-results ort-results/analyzer/analyzer-result.yml \
  --spdx-doc enhanced-spdx/bom-enhanced-fixed.spdx.json \
  --uncertain-packages uncertain-packages/uncertain-packages.json \
  --output curation-report-conflicts.html
```
- Only runs if uncertain packages exist
- Uses ScanCode-enhanced SPDX if available
- Generates focused conflict analysis

## GitHub Pages Deployment

Both reports are copied to the `public/` directory and deployed to GitHub Pages:

```
https://<username>.github.io/<repo>/
‚îú‚îÄ‚îÄ index.html (landing page with links to both reports)
‚îú‚îÄ‚îÄ curation-report-main.html (primary AI report)
‚îú‚îÄ‚îÄ curation-report-conflicts.html (conflict analysis, if needed)
‚îú‚îÄ‚îÄ scan-report-web-app.html (ORT WebApp)
‚îú‚îÄ‚îÄ bom.cyclonedx.json (CycloneDX SBOM)
‚îú‚îÄ‚îÄ bom-enhanced.spdx.json (Enhanced SPDX, if ScanCode ran)
‚îî‚îÄ‚îÄ uncertain-packages-report.md (markdown report)
```

## Required GitHub Secrets

### For Main Report Only:
```
AZURE_OPENAI_API_KEY
```

### For Both Reports (Recommended):
```
AZURE_OPENAI_API_KEY
AZURE_OPENAI_ENDPOINT
```

## What You'll See in Workflow Logs

### Check AI Curation Prerequisites:
```
üîç Checking prerequisites for AI curation...

Checking required files:
  ‚úì ORT analyzer results found
  ‚úì Uncertain packages found
  ‚úì SPDX document found

Checking Azure OpenAI configuration:
  ‚úì API key configured
  ‚úì Endpoint configured
```

### Generate Main ORT Curation Report:
```
ü§ñ Stage 5a: Generating main ORT curation report...
Running ort_curation_script_html.py...
‚úÖ Main curation report generated: curation-report-20250102-140530.html
   Size: 125847 bytes
‚úÖ Copied to curation-report-main.html for deployment
```

### Generate Enhanced Conflict Analysis:
```
ü§ñ Stage 5b: Generating enhanced conflict analysis...
Found 12 uncertain packages requiring analysis
Using enhanced SPDX: enhanced-spdx/bom-enhanced-fixed.spdx.json
Running enhanced_ai_curation.py...
‚úÖ Conflict analysis report generated (45123 bytes)
```

## Troubleshooting

### No curation report generated

**Check 1: Azure OpenAI Credentials**
```
‚ö†Ô∏è  AZURE_OPENAI_API_KEY not set, skipping AI report...
```
**Solution:** Add `AZURE_OPENAI_API_KEY` secret in GitHub repository settings

**Check 2: ORT Analyzer Failed**
```
‚ùå ORT analyzer results not found!
```
**Solution:** Check earlier workflow steps - ORT analyzer must complete successfully

**Check 3: API Errors**
Look in `ort-curation.log` or `conflict-analysis.log` for errors like:
- `Authentication failed` - Invalid API key
- `Resource not found` - Wrong endpoint or deployment name
- `Rate limit exceeded` - Too many requests

### Only one report generated

This is normal! The conflict analysis report only generates when:
1. Uncertain packages are detected AND
2. Azure OpenAI is fully configured (both key and endpoint)

If all your packages have clear licenses, only the main report will be generated (which is good!).

### Reports are small (< 10KB)

The reports might be fallback/error pages. Check the workflow logs for:
```
‚ö†Ô∏è  No curation report generated, check ort-curation.log
```

Then examine the log files for the specific error.

## Next Steps

1. **Push the updated workflow:**
   ```bash
   git add enhanced-ort-workflow.yml generate_landing_page.py
   git commit -m "Integrate original ORT curation script + enhanced conflict analysis"
   git push
   ```

2. **Verify Azure OpenAI secrets are set:**
   - Go to GitHub repository ‚Üí Settings ‚Üí Secrets and variables ‚Üí Actions
   - Confirm `AZURE_OPENAI_API_KEY` exists
   - Optionally add `AZURE_OPENAI_ENDPOINT` (or script will use default)

3. **Watch the workflow run:**
   - Check "Check AI curation prerequisites" step
   - Watch for successful report generation
   - Download artifacts if needed

4. **View reports on GitHub Pages:**
   - Wait for deployment to complete
   - Visit `https://<username>.github.io/<repo>/`
   - Click on "AI Curation Report" (main) and "License Conflict Analysis" (if applicable)

## Model Configuration

### Main Report (ort_curation_script_html.py):
- **Model**: `gpt-4.1-mini`
- **Temperature**: 0.3 (deterministic)
- **Max Tokens**: 4000
- **Deployment**: Configured in script line 576

### Conflict Analysis (enhanced_ai_curation.py):
- **Model**: `gpt-4`
- **Temperature**: 0.3 (deterministic)
- **Max Tokens**: 1000 per conflict
- **Limit**: First 20 conflicts to avoid token limits

## Cost Optimization

**Main Report:**
- Runs once per workflow
- Analyzes all packages
- ~3000-4000 tokens output

**Conflict Analysis:**
- Only runs if needed
- Limited to 20 conflicts max
- ~1000 tokens per conflict

**Total estimated cost per run:**
- With conflicts: ~$0.15 - $0.30 USD
- Without conflicts: ~$0.05 - $0.10 USD
- (Based on GPT-4 pricing as of 2025)

## Success Criteria

‚úÖ **Fully Working System:**
```
‚úì ORT analyzer completes
‚úì Main curation report generated (>100KB)
‚úì Uncertain packages detected (if applicable)
‚úì Conflict analysis generated (if needed)
‚úì Both reports deployed to GitHub Pages
‚úì Landing page shows both reports
```

## Support

If issues persist:
1. Check workflow logs for "Check AI curation prerequisites"
2. Review `ort-curation.log` and `conflict-analysis.log` outputs
3. Verify Azure OpenAI deployment name matches your configuration
4. Test scripts locally with sample data
